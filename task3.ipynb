{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "from scipy.stats import multivariate_normal \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, k, max_iter=5):\n",
    "        self.k = k\n",
    "        self.max_iter = int(max_iter) \n",
    "\n",
    "    def initialize(self, X):\n",
    "        # returns the (r,c) value of the numpy array of X\n",
    "        self.shape = X.shape \n",
    "        # n has the number of rows while m has the number of columns of dataset X\n",
    "        self.n, self.m = self.shape \n",
    "        \n",
    "\n",
    "        # initial weights given to each cluster are stored in phi or P(Ci=j)\n",
    "        self.phi = np.full(shape=self.k, fill_value=1/self.k) \n",
    "\n",
    "        # initial weights given to each data point wrt to each cluster or P(Xi/Ci=j)\n",
    "        self.weights = np.full(shape=self.shape, fill_value=1/self.k)\n",
    "        \n",
    "        # dataset is divided randomly into k parts of unequal sizes\n",
    "        random_row = np.random.randint(low=0, high=self.n, size=self.k)\n",
    "\n",
    "        # initial value of mean of k Gaussians\n",
    "        self.mu = [  X[row_index,:] for row_index in random_row ] \n",
    "\n",
    "        # initial value of covariance matrix of k Gaussians\n",
    "        self.sigma = [ np.cov(X.T) for _ in range(self.k) ] \n",
    "        # theta =(mu1,sigma1,mu2,simga2......muk,sigmak)\n",
    "\n",
    "    # E-Step: update weights and phi holding mu and sigma constant\n",
    "    def e_step(self, X):\n",
    "        # updated weights or P(Xi/Ci=j)\n",
    "        self.weights = self.predict_proba(X)\n",
    "        # mean of sum of probability of all data points wrt to one cluster is new updated probability of cluster k or (phi)k\n",
    "        self.phi = self.weights.mean(axis=0)\n",
    "\n",
    "    # M-Step: update meu and sigma holding phi and weights constant\n",
    "    def m_step(self, X):\n",
    "        for i in range(self.k):\n",
    "            weight = self.weights[:, [i]]\n",
    "            total_weight = weight.sum()\n",
    "\n",
    "            self.mu[i] = (X * weight).sum(axis=0) / total_weight\n",
    "            self.sigma[i] = np.cov(X.T,aweights=(weight/total_weight).flatten(), bias=True)\n",
    "\n",
    "    # responsible for clustering the data points correctly\n",
    "    def fit(self, X):\n",
    "        # initialise parameters like weights, phi, meu, sigma of all Gaussians in dataset X\n",
    "        self.initialize(X)\n",
    "        plt.figure(figsize=(16, 25))\n",
    "        for iteration in range(self.max_iter):\n",
    "            # iterate to update the value of P(Xi/Ci=j) and (phi)k\n",
    "            self.e_step(X)\n",
    "            # iterate to update the value of meu and sigma as the clusters shift\n",
    "            self.m_step(X)\n",
    "        return(self.mu, self.sigma)\n",
    "\n",
    "            \n",
    "\n",
    "    # predicts probability of each data point wrt each cluster\n",
    "    def predict_proba(self, X):\n",
    "        # Creates a n*k matrix denoting probability of each point wrt each cluster \n",
    "        likelihood = np.zeros( (self.n, self.k) ) \n",
    "        for i in range(self.k):\n",
    "            distribution = multivariate_normal(mean=self.mu[i],cov=self.sigma[i])\n",
    "            # pdf : probability denisty function\n",
    "            likelihood[:,i] = distribution.pdf(X) \n",
    "\n",
    "        numerator = likelihood * self.phi\n",
    "        denominator = numerator.sum(axis=1)[:, np.newaxis]\n",
    "        weights = numerator / denominator\n",
    "        return weights\n",
    "    \n",
    "    # predict function \n",
    "    def predict(self, X):\n",
    "        weights = self.predict_proba(X)\n",
    "        # datapoint belongs to cluster with maximum probability\n",
    "        # returns this value\n",
    "        return np.argmax(weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.array(Image.open(r\"dataset3\\testGrayImage_high.jpg\").convert(\"L\")).reshape(-1,1)\n",
    "gm = GMM(4,19)\n",
    "res = gm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([248.27679038]), array([108.0850869]), array([183.50250967]), array([12.84229714])], [array(46.95502929), array(1918.91840819), array(696.87867406), array(270.22411158)])\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    def __init__(self, K, mus, sigmas, weights, tol_level, image_path):\n",
    "        self.k = K # number of clusters\n",
    "        self.mus = mus # means k x 1\n",
    "        self.sigmas = sigmas # variances K x1\n",
    "        self. weights = weights # wight of each cluster k x1\n",
    "        self.tol_level = tol_level # the tolarance level\n",
    "        self.data = np.array(Image.open(image_path).convert(\"L\")).reshape(-1,1) #data points  n x 1\n",
    "        # self.data = np.array([[1,1,2,3,4,3,5,7,1,2,3,4]])\n",
    "        # self.data=  np.array(  [[1],\n",
    "        #                         [1],\n",
    "        #                         [2],\n",
    "        #                         [3],\n",
    "        #                         [4],\n",
    "        #                         [3],\n",
    "        #                         [5],\n",
    "        #                         [7],\n",
    "        #                         [1],\n",
    "        #                         [2],\n",
    "        #                         [3],\n",
    "        #                         [4]])\n",
    "        # print(self.data.shape)\n",
    "        self.probabilities = np.array([0])# the probability for each class for each point N X K (2)\n",
    "        \n",
    "\n",
    "    def e_step(self): # prob of each data point for each cluster N X K and the new classes weights\n",
    "        # we need the weights of the classes, parameters of the classes, data point\n",
    "        \n",
    "        temp = np.zeros( (self.data.shape[0], self.k)) # N X K\n",
    "        for i in range(self.k):\n",
    "            pd = scipy.stats.norm(self.mus[i],self.sigmas[i]).pdf(self.data)\n",
    "            temp[:,i] = pd.reshape(-1)\n",
    "        numerator = temp * self.weights.reshape(-1) # numerator done N X K (1)\n",
    "        denominator = numerator.sum(axis=1)[:, np.newaxis]\n",
    "        self.probabilities = numerator / denominator # the probability for each class for each point N X K (2)\n",
    "        self.probabilities[np.isnan(self.probabilities)] = 0\n",
    "        \n",
    "        self.weights = self.probabilities.mean(axis=0).reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def m_step(self):\n",
    "        for i in range(self.k):\n",
    "            weight = self.probabilities[:, [i]]\n",
    "            total_weight = weight.sum()\n",
    "            a1 = (self.data * weight).sum(axis=0)\n",
    "            a2 = total_weight\n",
    "            if a1 == 0 or a2 == 0:\n",
    "                self.mus[i] = 0\n",
    "            else:\n",
    "                self.mus[i] = a1 / a2\n",
    "        p1 = np.transpose(np.array([ self.data[:,0] for _ in range(0,self.k)]))\n",
    "        p2 = np.array([ self.mus.reshape(-1) for _ in range(0,self.data.shape[0])]) # n x k\n",
    "        p2 = np.square(p1 - p2)\n",
    "        p2 = p2 * self.probabilities\n",
    "        p2 = np.sum(p2,axis=0).reshape(-1,1)\n",
    "        total_weight = np.sum(self.probabilities, axis=0).reshape(-1,1)\n",
    "        p2 = p2 / total_weight\n",
    "        p2[np.isnan(p2)] = 0\n",
    "        self.sigmas = p2.reshape(-1,1)\n",
    "        \n",
    "    def q_objecive(self,old_mus, old_sigmas,old_weights):\n",
    "        temp = np.zeros( (self.data.shape[0], self.k)) # N X K\n",
    "        for i in range(self.k):\n",
    "            pd = scipy.stats.norm(self.mus[i],self.sigmas[i]).pdf(self.data)\n",
    "            temp[:,i] = pd.reshape(-1)\n",
    "        temp1 = np.log(self.probabilities * temp)\n",
    "        temp1[np.isnan(temp1)] = 0\n",
    "        for i in range(self.k):\n",
    "            pd = scipy.stats.norm(old_mus[i],old_sigmas[i]).pdf(self.data)\n",
    "            temp[:,i] = pd.reshape(-1)\n",
    "        \n",
    "        numerator = temp * old_weights.reshape(-1)\n",
    "        denominator = numerator.sum(axis=1)[:, np.newaxis]\n",
    "        temp = numerator / denominator # the probability for each class for each point N X K (previous) \n",
    "        temp[np.isnan(temp)] = 0\n",
    "        temp = temp * temp1\n",
    "        temp = np.sum(temp, axis=1)\n",
    "        temp = np.sum(temp, axis=0)\n",
    "        if temp < self.tol_level:\n",
    "            return True\n",
    "        # print(temp)\n",
    "        return False\n",
    "        \n",
    "    def cluster(self):\n",
    "        stop = False\n",
    "        self.e_step\n",
    "        self.m_step\n",
    "        itrs = 0\n",
    "        while stop == False:\n",
    "        # for i in range(0,1):\n",
    "            itrs+=1\n",
    "            old_mus = np.array(self.mus)\n",
    "            old_sigs = np.array(self.sigmas)\n",
    "            old_weights = np.array(self.weights)\n",
    "            self.e_step()\n",
    "            self.m_step()\n",
    "            print(itrs)\n",
    "            stop = self.q_objecive(old_mus, old_sigs, old_weights)\n",
    "        return(self.mus, self.sigmas)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "phi = np.full(shape=k, fill_value=1/k) \n",
    "mu = np.array([[25],\n",
    "               [90],\n",
    "               [160],\n",
    "               [240]])\n",
    "sigma = np.array([[1],\n",
    "               [1],\n",
    "               [1],\n",
    "               [1]])\n",
    "e = 0.01\n",
    "\n",
    "em1 = EM(k, mu, sigma, phi, e, r\"dataset3\\testGrayImage.jpg\")\n",
    "# em1.test()\n",
    "results = em1.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  0],\n",
      "       [ 85],\n",
      "       [170],\n",
      "       [255]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]))\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d0bf86eb3e3037358ba9c78ec3f827f641a1e9419297de3af080288153d82b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
